{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "229e3fcd-344e-42a6-b527-ba32995f0579",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_video_clip_VGG16(video_path):\n",
    "    \"\"\"\n",
    "    Preprocess a video clip:\n",
    "    - Extract frames\n",
    "    - Resize frames to 224x224\n",
    "    Args:\n",
    "    - video_path: Path to the video clip\n",
    "    Returns:\n",
    "    - A list of preprocessed frames\n",
    "    \"\"\"\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    frames = []\n",
    "    n =0\n",
    "    # Read frames from the video\n",
    "    while cap.isOpened():\n",
    "        n = n+1\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        print(f\"\\r\\t...extracting and resizing frame: {n}\", end='', flush=True)\n",
    "        \n",
    "        # Resize the frame to 224x224\n",
    "        resized_frame = cv2.resize(frame, (224, 224))\n",
    "        \n",
    "        frames.append(resized_frame)\n",
    "    \n",
    "    # Release the video capture object\n",
    "    cap.release()\n",
    "    print(\" of\", len(frames), \"frames\")\n",
    "    \n",
    "    print(f\"\\n\\n   SUMMARY: \\n\\tThe clip {video_path} has been transformed in frames with the right format\", flush=True)\n",
    "    print(f\"\\t    Total frames created: {len(frames)}\", flush=True)\n",
    "    print(f\"\\t    Frame format: 'size=224x224', 'color=RGB'\", flush=True)\n",
    "\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e37bbc6e-7e0e-4904-bf8f-88a2eaba0ed6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_single_clip_data_VGG16(clip_frames):\n",
    "    \"\"\"\n",
    "    Prepare data for a single video clip using VGG16.\n",
    "    \n",
    "    Args:\n",
    "    - clip_frames: List of frames from a single video clip.\n",
    "    \n",
    "    Returns:\n",
    "    - A sequence of features for the given video clip.\n",
    "    \"\"\"\n",
    "    \n",
    "    print('\\t...loading the VGG16 pre-trained model', flush=True)\n",
    "    # Load VGG16 model without the top classification layers\n",
    "    base_model = VGG16(weights='imagenet', include_top=False)\n",
    "    feature_extractor = Model(inputs=base_model.input, outputs=base_model.layers[-1].output)\n",
    "    \n",
    "    clip_features = []  # List to store features for each frame\n",
    "\n",
    "    for i, frame in enumerate(clip_frames):\n",
    "        print(f\"\\r\\t...resizing, converting to RGB, normalising and extracting features from frame: {i+1:3} of {len(clip_frames)} \", end='', flush=True)\n",
    "        \n",
    "        # Preprocess the frame for VGG16\n",
    "        frame = preprocess_input(frame)  \n",
    "        frame = np.expand_dims(frame, axis=0)   # Add batch dimension\n",
    "\n",
    "        # Extract features\n",
    "        feature_vector = feature_extractor.predict(frame)\n",
    "        \n",
    "         # Flatten the feature vector\n",
    "        flattened_vector = np.reshape(feature_vector, (25088,))\n",
    "        \n",
    "        clip_features.append(flattened_vector)\n",
    "        \n",
    "    print('\\n')\n",
    "    print(f\"\\n   SUMMARY: \\n\\tThe {len(clip_frames)} frames have been transformed into a vector of features through the VGG16 model\", flush=True)\n",
    "    print(f\"\\t    Total vectors created: '1'\", flush=True)\n",
    "    print(f\"\\t    Total frames: '{np.array(clip_features).shape[0]}'\", flush=True)\n",
    "    print(f\"\\t    Total feature per frame: '{np.array(clip_features).shape[1]}' (as per VGG16 output)\", flush=True)\n",
    "    print(f\"\\t    Reality check: \\n\\t\\t'Print shape of resulting array' = {np.array(clip_features).shape}\", flush=True)\n",
    "    return [clip_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f1c04558-c4f0-474c-b886-8bdeb3fab377",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transcribe_audio(file_path, speech_recognition_lang):\n",
    "    r = sr.Recognizer()\n",
    "    with sr.AudioFile(file_path) as source:\n",
    "        audio_data = r.record(source)\n",
    "        try:\n",
    "            # Set the language to Italian using the `language` parameter\n",
    "            text = r.recognize_google(audio_data, language=speech_recognition_lang)\n",
    "            print( '\\t...transcritpion extracted as:', \"'\",text,\"'\")\n",
    "            return text\n",
    "        except sr.UnknownValueError:\n",
    "            return \"Google Speech Recognition could not understand audio\"\n",
    "        except sr.RequestError:\n",
    "            return \"API unavailable or unresponsive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1cd386db-1be2-4a07-bfbf-fa20adfeb6d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def extract_audio_from_video(video_path, audio_output_path):\n",
    "    \"\"\"\n",
    "    Extracts audio from the given video and saves it as a .wav file using pydub.\n",
    "    \n",
    "    Parameters:\n",
    "    - video_path: Path to the input video file.\n",
    "    - audio_output_path: Path to save the extracted audio.\n",
    "    \"\"\"\n",
    "    print('\\t...extracting the audio from the video:', video_path)\n",
    "    audio = AudioSegment.from_file(video_path, format=\"avi\")\n",
    "    audio.export(audio_output_path, format=\"wav\")\n",
    "    print('\\t...audio saved successfully on a temp .wav file')\n",
    "          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ae5f981-c9d5-40b9-9540-12967fbf9f53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_to_mp4(avi_path):\n",
    "    \"\"\"Converts the given .avi video to .mp4 format.\"\"\"\n",
    "    print('\\t...converting in compatible format')\n",
    "    mp4_path = avi_path.replace(\".avi\", \".mp4\")\n",
    "    os.system(f\"ffmpeg -i {avi_path} {mp4_path}\")\n",
    "    return mp4_path\n",
    "\n",
    "def display_video_clip(clip_path):\n",
    "    \"\"\"Displays the video clip in the notebook. Converts .avi to .mp4 if necessary.\"\"\"\n",
    "    if clip_path.endswith(\".avi\"):\n",
    "        clip_path = convert_to_mp4(clip_path)\n",
    "    print('\\n')\n",
    "    display(Video(clip_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d73a4544-9ca4-4342-8151-20d2ba478979",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_audio_from_video(video_path, speech_recognition_lang, translation_lang):\n",
    "    \"\"\"\n",
    "    Processes the audio from the given video clip:\n",
    "    1. Extracts audio from the video.\n",
    "    2. Transcribes the audio.\n",
    "    3. Detects the language of the transcription.\n",
    "    4. Translates the transcription to English if it's not already in English.\n",
    "    \n",
    "    Parameters:\n",
    "    - video_path: Path to the video file.\n",
    "    \n",
    "    Returns:\n",
    "    - Transcription in English.\n",
    "    \"\"\"\n",
    "    # Extract audio from the video\n",
    "    audio_output_path = \"temp_audio.wav\"  # Temporary path to save the extracted audio\n",
    "    extract_audio_from_video(video_path, audio_output_path)\n",
    "    \n",
    "    # Transcribe the audio and detect its language\n",
    "    transcription = transcribe_audio(audio_output_path, speech_recognition_lang)\n",
    "    detected_language = translation_lang\n",
    "    # If the detected language is not English, translate the transcription to English\n",
    "    if detected_language and detected_language != 'en':\n",
    "        translator = Translator()\n",
    "        translation = translator.translate(transcription, src=detected_language, dest='en').text\n",
    "        print('\\t... text translated as:', \"'\",translation,\"'\")\n",
    "        res =  translation\n",
    "    else:\n",
    "        res = transcription\n",
    "        \n",
    "        # Delete the temporary audio file\n",
    "    if os.path.exists(audio_output_path):\n",
    "        print(\"\\t...deleting temporary audio file\", flush=True)\n",
    "        os.remove(audio_output_path)\n",
    "        \n",
    "    \n",
    "    while True:\n",
    "        # Ask the user if they want to view the clip\n",
    "        view_clip = input(\"\\n\\tDo you want to view the video clip for verification? (yes/no): \").lower()\n",
    "\n",
    "        if view_clip == \"yes\":\n",
    "            display_video_clip(video_path)\n",
    "            input(\"\\tPress Enter once you're done watching the video...\")  # Pause execution\n",
    "            break\n",
    "        elif view_clip == \"no\":\n",
    "            print(\"\\tOkay, proceeding without displaying the video clip.\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"Invalid response. Please answer with 'yes' or 'no'.\")\n",
    "        \n",
    "    print(f\"\\n   SUMMARY: \\n\\tThe audio of the clip: {video_path} has been captured and translated\", flush=True)\n",
    "    print(f\"\\t    Input language: '{translation_lang}'\", flush=True)\n",
    "    print(f\"\\t    Translated language: 'en'\", flush=True)\n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b660485b-6b5d-459f-bd63-383f7225646d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_text(data):\n",
    "\n",
    "    data=re.sub(r\"(#[\\d\\w\\.]+)\", '', data)\n",
    "    data=re.sub(r\"(@[\\d\\w\\.]+)\", '', data)\n",
    "    print(\"\\t...'#' and '@' removed from the text\", flush=True)\n",
    "    \n",
    "    with open('Models/tokenizer_new.pickle', 'rb') as handle:\n",
    "        tokenizer = pickle.load(handle)\n",
    "    print('\\t...tokenizer:', tokenizer, 'has been loaded', flush=True)\n",
    "    \n",
    "     # Tokenize the text\n",
    "    text_sequences = tokenizer.texts_to_sequences([data])\n",
    "    print('\\t...the text has been tokenized into sequences', flush=True)\n",
    "    # Pad the sequences to the desired length\n",
    "    padded_text_sequences = pad_sequences(text_sequences, maxlen=500)\n",
    "    print(\"\\t...the text has been padded with 'PRE' 500 characters\")\n",
    "    \n",
    "    print(f\"\\n   SUMMARY: \\n\\tThe text has been sanitized and preprocessed to meet the model input requirements\", flush=True)\n",
    "    print(f\"\\t    Tokenizer used: '{tokenizer}' (as per trained model)\", flush=True)\n",
    "    print(f\"\\t    sequences shape '{padded_text_sequences.shape}' (as per trained model)\", flush=True)\n",
    "    print(f\"\\t    sequence example {padded_text_sequences[0]}\", flush=True)\n",
    "    \n",
    "    return padded_text_sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b6ece8ff-9362-4a9b-a9be-8196b96db5fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " def run_multi_input_VGG16(video_sequences, transcriptions, emotion_labels):\n",
    "    print(\"Video Sequences\",np.array(video_sequences).shape)\n",
    "    print('\\t...importing pre-trained rnn model for image stream input', flush=True)\n",
    "    # Load the pre-trained CNN-LSTM model for video frames\n",
    "    video_model_path = 'Models/best_rnn_model.h5'  # Replace with the correct path to your model\n",
    "    video_model = load_model(video_model_path)\n",
    "    \n",
    "    print('\\n')\n",
    "    print('*'*70)\n",
    "    video_model.summary()\n",
    "    print('*'*70, '\\n')\n",
    "    input(\"\\tPress Enter once you're done checking the model...\")  # Pause execution\n",
    "    print('\\n\\t...importing pre-trained rnn model for text stream input', flush=True)\n",
    "    # Load the pre-trained text classifier model for audio transcription\n",
    "    text_model_path = 'Models/new_textmodel.h5'  # Replace with the correct path to your model\n",
    "    text_model = load_model(text_model_path)\n",
    "    # Check the models' architectures\n",
    "    print('\\n')\n",
    "    print('*'*70)\n",
    "    text_model.summary()\n",
    "    print('*'*70, '\\n')\n",
    "    input(\"\\tPress Enter once you're done checking the model...\")  # Pause execution\n",
    "    print('\\n\\t...creating our Multi_Input model', flush=True)\n",
    "    \n",
    "    # Define the video input shape\n",
    "    num_frames = None  # Variable number of frames\n",
    "    feature_vector_size = 25088\n",
    "    # Define the video input\n",
    "    video_frames_input = Input(shape=(num_frames, feature_vector_size))\n",
    "    video_output = video_model(video_frames_input)\n",
    "\n",
    "    # Define the text input shape\n",
    "    text_sequence_length = 500  # This is the max length you've used for padding\n",
    "\n",
    "    # Define the text input\n",
    "    text_input = Input(shape=(text_sequence_length,))\n",
    "    text_output = text_model(text_input)\n",
    "\n",
    "    # Compute the Cosine Similarity\n",
    "    similarity = Dot(axes=1, normalize=True)([video_output, text_output])\n",
    "\n",
    "    # Optionally, pass the similarity through a dense layer to get a value between 0 and 1\n",
    "    similarity_score = Dense(1, activation='sigmoid')(similarity)\n",
    "\n",
    "    # Define the multi-input model\n",
    "    multi_input_model = Model(inputs=[video_frames_input, text_input], outputs=similarity_score, name=\"Our_MultiInputModel\")\n",
    "\n",
    "    # Compile the model\n",
    "    multi_input_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    print('\\n')\n",
    "    print('*'*70)\n",
    "    multi_input_model.summary()\n",
    "    print('Activation: Sigmoid', '\\nOptimizer: Adam', '\\nLoss: loss: categorical_crossentropy', '\\nSimilarity: Dot')\n",
    "    print('*'*70, '\\n')\n",
    "    input(\"\\tPress Enter once you're done checking the model...\")  # Pause execution\n",
    "    print('\\n\\t...predicting Similarity', flush=True)\n",
    "    # Predict the similarity score\n",
    "    # Assuming you have `sequence` and `tokenized_padded_text` ready\n",
    "    \n",
    "    # video_sequences = np.expand_dims(video_sequences, axis=0)  # Ensure it has shape (1, ...)\n",
    "    transcriptions = np.expand_dims(transcriptions, axis=0)  # Ensure it has shape (1, ...)\n",
    "\n",
    "    similarity_prediction = multi_input_model.predict([np.array(video_sequences), np.array(transcriptions)])\n",
    "    \n",
    "    print('\\t...evaluating results', flush=True)\n",
    "    # Define the thresholds and corresponding verbal levels\n",
    "    thresholds = [0.2, 0.4, 0.6, 0.8]\n",
    "    levels = [\"Completely Unmatched\", \"Slightly Matched - \", \"Moderately Matched\", \"Highly Matched\", \"Perfect Match\"]\n",
    "\n",
    "    # Determine the verbal level based on the similarity score\n",
    "    level = levels[0]  # Default to \"Completely Unmatched\"\n",
    "    for i, threshold in enumerate(thresholds):\n",
    "        if similarity_prediction[0] > threshold:\n",
    "            level = levels[i+1]\n",
    "    print('\\n')\n",
    "    print('*'*70)\n",
    "    print(\"    Predicted Similarity Score: {:.4f} - {}\".format(similarity_prediction[0].item(), level))\n",
    "    print('*'*70)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
