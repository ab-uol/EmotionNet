{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f5946fb-a333-4eae-9754-b83978f93b10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_frames_from_folder(subject_folder_path):\n",
    "    \"\"\"\n",
    "    Extract frames from all .avi video clips in the specified folder and save them in a 'Separated_Frames' subfolder.\n",
    "    \n",
    "    Parameters:\n",
    "    - subject_folder_path: Path to the folder containing .avi video clips.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure the provided subject folder exists\n",
    "    if not os.path.exists(subject_folder_path):\n",
    "        print(f\"Error: The folder '{subject_folder_path}' does not exist.\")\n",
    "        return\n",
    "\n",
    "    # Retrieve all .avi files from the subject folder\n",
    "    avi_files = [f for f in os.listdir(subject_folder_path) if f.endswith('.avi')]\n",
    "    \n",
    "    # Print the total number of clips found in the folder\n",
    "    print(f\"\\t...total clips in the folder: {len(avi_files)}\")\n",
    "\n",
    "    # If no .avi files are found, notify the user and exit\n",
    "    if not avi_files:\n",
    "        print(f\"No .avi files found in the folder '{subject_folder_path}'.\")\n",
    "        return\n",
    "\n",
    "    # Create a subfolder named 'Separated_Frames' to store the extracted frames\n",
    "    output_folder = os.path.join(subject_folder_path, 'Separated_Frames')\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "        print(f\"\\t...creating a new folder to store frames from clips at: {output_folder}\")\n",
    "\n",
    "    # Initialize a counter for total frames extracted\n",
    "    all_frames = 0\n",
    "\n",
    "    # Loop through each .avi file to extract frames\n",
    "    for n, avi_file in enumerate(avi_files):\n",
    "        video_path = os.path.join(subject_folder_path, avi_file)\n",
    "        \n",
    "        # Double-check if the video file exists\n",
    "        if not os.path.exists(video_path):\n",
    "            print(f\"Error: The video file '{video_path}' does not exist.\")\n",
    "            continue\n",
    "        \n",
    "        # Create a sub-folder for each video's frames inside the 'Separated_Frames' folder\n",
    "        video_name = os.path.splitext(avi_file)[0]  # Extract the video name without the .avi extension\n",
    "        video_output_folder = os.path.join(output_folder, video_name)\n",
    "        \n",
    "        if not os.path.exists(video_output_folder):\n",
    "            os.makedirs(video_output_folder)\n",
    "\n",
    "        # Use OpenCV to open the video file\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "        # If the video cannot be opened, notify the user and continue to the next video\n",
    "        if not cap.isOpened():\n",
    "            print(f\"Error: Unable to open the video file '{video_path}'.\")\n",
    "            continue\n",
    "\n",
    "        # Determine the total number of frames in the video\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "        # Extract each frame and save it as an image\n",
    "        for frame_num in range(total_frames):\n",
    "            all_frames += 1\n",
    "            print(f\"\\r\\t...extracting frame: {frame_num} from clip: {n} of {len(avi_files)}  \", end='', flush=True)\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(f\"Error: Unable to read frame {frame_num} from video '{video_path}'.\")\n",
    "                break\n",
    "            \n",
    "            # Define the path for saving the extracted frame\n",
    "            output_image_path = os.path.join(video_output_folder, f\"frame_{frame_num:04d}.png\")\n",
    "            cv2.imwrite(output_image_path, frame)\n",
    "\n",
    "        # Release the video capture object to free up resources\n",
    "        cap.release()\n",
    "\n",
    "    # Print a summary of the frame extraction process\n",
    "    example_frame_name = os.path.join(video_output_folder, \"frame_0000.png\")\n",
    "    print(f\"\\n\\n   SUMMARY: \\n\\tExtracted {all_frames} frames from the folder '{subject_folder_path}' and saved in '{output_folder}'\")\n",
    "    print(f\"\\tTotal clips extracted: {len(avi_files)}\")\n",
    "    print(f\"\\tExample frame name: {example_frame_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec5548d1-cb3a-48d6-a762-f4bf9b016b43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_data_from_frames(base_folder, emotion_to_int, emotion_labels):\n",
    "    \"\"\"\n",
    "    Prepare data from frames using a pre-trained CNN model.\n",
    "    \n",
    "    Parameters:\n",
    "    - base_folder: Path to the folder containing extracted frames.\n",
    "    - emotion_to_int: Dictionary mapping emotion names to integer labels.\n",
    "    - emotion_labels: List of emotion labels.\n",
    "    \n",
    "    Returns:\n",
    "    - sequences: List of feature sequences extracted from frames.\n",
    "    - labels: Numpy array of labels corresponding to the sequences.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the pre-trained CNN model\n",
    "    print('\\t...loading our best pre-trained model', flush=True)\n",
    "    loaded_model = load_model('Models/fer_model2_6emotions.h5')\n",
    "\n",
    "    # Remove the final classifier layers to use the model as a feature extractor\n",
    "    print('\\t...removing the final classifier layers from the model', flush=True)\n",
    "    feature_extractor = Model(inputs=loaded_model.input, outputs=loaded_model.layers[-2].output)\n",
    "\n",
    "    # Point to the 'Separated_Frames' subfolder where the extracted frames are stored\n",
    "    frames_folder = os.path.join(base_folder, 'Separated_Frames')\n",
    "    emotion_folders = sorted(os.listdir(frames_folder))\n",
    "    \n",
    "    print(f\"\\t...found {len(emotion_folders)} folders of frames, each from a clip\", flush=True)\n",
    "    \n",
    "    # Initialize lists to store sequences of features and corresponding labels\n",
    "    sequences = []\n",
    "    labels = []\n",
    "\n",
    "    # Loop through each emotion folder to process the frames\n",
    "    for idx, emotion_folder in enumerate(emotion_folders, 1):\n",
    "        emotion_folder_path = os.path.join(frames_folder, emotion_folder)\n",
    "        \n",
    "        # Ensure the path is a directory\n",
    "        if not os.path.isdir(emotion_folder_path):\n",
    "            continue\n",
    "\n",
    "        # List all the frame files in the current emotion folder\n",
    "        clip_files = sorted(os.listdir(emotion_folder_path))\n",
    "        clip_features = []  # List to store features for each frame in the current clip\n",
    "        \n",
    "        # Process each frame in the current emotion folder\n",
    "        for file_idx, clip_file in enumerate(clip_files):\n",
    "            print(f\"\\r\\t\\t...of file {file_idx+1}/{len(clip_files)} in folder {idx}, called {emotion_folder}\", end='', flush=True)\n",
    "            \n",
    "            clip_path = os.path.join(emotion_folder_path, clip_file)\n",
    "            frame = cv2.imread(clip_path, cv2.IMREAD_GRAYSCALE)\n",
    "            \n",
    "            # Ensure the frame is read correctly\n",
    "            if frame is None:\n",
    "                print(f\"\\tError reading image: {clip_path}\")\n",
    "                continue\n",
    "\n",
    "            # Resize the frame to match the input size of the CNN model\n",
    "            frame = cv2.resize(frame, (48, 48))\n",
    "        \n",
    "            # Normalize the pixel values to [0, 1] range\n",
    "            frame = frame / 255.0\n",
    "            frame = np.expand_dims(frame, axis=-1)  # Add channel dimension\n",
    "            frame = np.expand_dims(frame, axis=0)   # Add batch dimension\n",
    "\n",
    "            # Extract features using the pre-trained CNN model\n",
    "            feature_vector = feature_extractor.predict(frame)\n",
    "            clip_features.append(feature_vector)\n",
    "\n",
    "        # Append the sequence of features and the corresponding label to the lists\n",
    "        sequences.append(clip_features)\n",
    "        labels.append(emotion_to_int[map_to_emotion(emotion_folder)])\n",
    "\n",
    "    # Print a summary of the data preparation process\n",
    "    print(f\"\\n\\n   SUMMARY: \\n\\tProcessed all emotion folders from '{frames_folder}'\", flush=True)\n",
    "    print(f\"\\t    Total folders processed: {len(sequences)}\", flush=True)\n",
    "    print(f\"\\t    Labels assigned: {labels}\", flush=True)\n",
    "\n",
    "    # Return the sequences of features and the corresponding labels\n",
    "    return sequences, np.array(labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86ef5796-0039-447b-970c-2e203aba7ae0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_jitter(sequence, factor=0.05):\n",
    "    # Add some jitter to the sequence for data augmentation\n",
    "    noise = np.random.normal(0, factor, sequence.shape)\n",
    "    return sequence + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c910daeb-a41a-429e-9527-28f7763e0f7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomPrintCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        train_acc = logs.get('accuracy')\n",
    "        val_acc = logs.get('val_accuracy')\n",
    "        print(f\"\\r\\t\\tEpoch {epoch+1} - train_accuracy: {train_acc:.4f} - val_accuracy: {val_acc:.4f}\", end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99e50f6c-c90e-448d-96fe-0cfb9aebe3db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_lstm(sequences, y_train_labels, emotion_labels):\n",
    "    \"\"\"\n",
    "    Train an LSTM model on the provided sequences and labels.\n",
    "    \n",
    "    Parameters:\n",
    "    - sequences: List of sequences of feature vectors.\n",
    "    - y_train_labels: Corresponding labels for the sequences.\n",
    "    - emotion_labels: List of emotion labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initial preprocessing step\n",
    "    print('\\t...preprocessing inputs', flush=True)\n",
    "\n",
    "    # Pad the sequences to ensure they have uniform length\n",
    "    print(\"\\t\\t...padding sequences of features with 'post'\", flush=True)\n",
    "    padded_sequences = pad_sequences(sequences, padding='post', dtype='float32')\n",
    "\n",
    "    # Convert the list of sequences into a numpy array for efficient computation\n",
    "    sequences_array = np.array(padded_sequences)\n",
    "    \n",
    "    # Convert the labels into one-hot encoded format for multi-class classification\n",
    "    print('\\t\\t...transforming labels with one-hot encoding', flush=True)\n",
    "    y_train_onehot = to_categorical(y_train_labels, num_classes=len(emotion_labels))\n",
    "    \n",
    "    # Provide an option for the user to verify the inputs\n",
    "    verify_choice = input(\"\\nWould you like to verify the inputs before proceeding? (yes/no): \").lower()\n",
    "    \n",
    "    # If the user chooses to verify, provide various methods of verification\n",
    "    if verify_choice == \"yes\":\n",
    "        while True:\n",
    "            print(\"\\n\\tChoose a method of verification:\")\n",
    "            print(\"\\t1. Display a summary of the data (shape, data type).\")\n",
    "            print(\"\\t2. Display the first few rows of the data.\")\n",
    "            print(\"\\t3. Display random samples from the data.\")\n",
    "            print(\"\\t4. Display basic statistics for the data.\")\n",
    "            print(\"\\t5. Proceed without further verification.\")\n",
    "            \n",
    "            verification_method = input(\"Enter your choice (1/2/3/4/5): \")\n",
    "            \n",
    "            # Display the chosen verification method's output\n",
    "            if verification_method == \"1\":\n",
    "                print(\"\\n\\t\\tSequences shape:\", sequences_array.shape)\n",
    "                print(\"\\t\\tOne-hot encoded labels shape:\", y_train_onehot.shape)\n",
    "            elif verification_method == \"2\":\n",
    "                print(\"\\n\\t\\tFirst few rows of sequences:\\n\", sequences_array[:5])\n",
    "                print(\"\\n\\t\\tFirst few rows of one-hot encoded labels:\\n\", y_train_onehot[:5])\n",
    "            elif verification_method == \"3\":\n",
    "                random_idx = np.random.randint(0, len(sequences_array), 5)\n",
    "                print(\"\\n\\t\\tRandom samples from sequences:\\n\", sequences_array[random_idx])\n",
    "                print(\"\\n\\t\\tRandom samples from one-hot encoded labels:\\n\", y_train_onehot[random_idx])\n",
    "            elif verification_method == \"4\":\n",
    "                print(\"\\n\\t\\tSequences mean:\", np.mean(sequences_array))\n",
    "                print(\"\\t\\tSequences median:\", np.median(sequences_array))\n",
    "                print(\"\\t\\tSequences standard deviation:\", np.std(sequences_array))\n",
    "            elif verification_method == \"5\":\n",
    "                break\n",
    "            else:\n",
    "                print(\"\\nInvalid choice. Please choose again.\")\n",
    "    \n",
    "    # Define the LSTM model's architecture\n",
    "    print('\\t...creating the model', flush=True)\n",
    "    input_shape = (None, 256)  # Variable sequence length with 256 features per frame\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = LSTM(128, return_sequences=False)(inputs)  # LSTM layer with 128 units\n",
    "    x = Dropout(0.5)(x)  # Dropout layer for regularization\n",
    "    outputs = Dense(len(emotion_labels), activation='softmax')(x)  # Final classification layer\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs, name=\"Our_Chosen_RNN_Model\")\n",
    "\n",
    "    # Compile the model with the Adam optimizer and categorical cross-entropy loss\n",
    "    optimizer = Adam(learning_rate=0.0005)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Display the model's summary\n",
    "    print('\\n')\n",
    "    print('*'*70)\n",
    "    model.summary()\n",
    "    print('Dropout: 0.5', '\\nActivation: SoftMax', '\\nOptimizer: Adam(learning_rate=0.0005)', '\\nLoss: loss: categorical_crossentropy')\n",
    "    print('*'*70, '\\n')\n",
    "    \n",
    "    # Implement data augmentation using the 'jitter' technique\n",
    "    print(\"\\t...implementing data-augmentation with 'jitter'\", end='', flush=True)\n",
    "    X_augmented = [add_jitter(seq) for seq in sequences_array]\n",
    "    X_augmented = np.array(X_augmented)\n",
    "    X_augmented = X_augmented.reshape(X_augmented.shape[0], X_augmented.shape[1], -1)\n",
    "    \n",
    "    # Train the model using k-fold cross-validation\n",
    "    print('\\n\\t...training the model with k-fold validation', end='', flush=True)\n",
    "    n_splits = 5\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    best_val_accuracy = 0\n",
    "    best_model = None\n",
    "    best_fold = 0\n",
    "    accuracies = []\n",
    "    fold = 1\n",
    "    \n",
    "    custom_print_callback = CustomPrintCallback()\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    \n",
    "    # Loop through each fold, train the model, and store the best model\n",
    "    for train_index, val_index in kf.split(X_augmented):\n",
    "        print(f\"\\n\\t   Training on fold {fold}/{n_splits}\")\n",
    "        X_train_fold, X_val_fold = X_augmented[train_index], X_augmented[val_index]\n",
    "        y_train_fold, y_val_fold = y_train_onehot[train_index], y_train_onehot[val_index]\n",
    "\n",
    "        history = model.fit(X_train_fold, y_train_fold, epochs=100, batch_size=16, validation_data=(X_val_fold, y_val_fold), verbose=0,\n",
    "                            callbacks=[custom_print_callback, early_stopping])\n",
    "\n",
    "        val_accuracy = history.history['val_accuracy'][-1]\n",
    "        accuracies.append(val_accuracy)\n",
    "\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            best_model = model\n",
    "            best_fold = fold\n",
    "        fold += 1\n",
    "        \n",
    "    # Save the best model after all folds are completed\n",
    "    print(f'\\n\\t...saving the best model from fold: {best_fold} with accuracy: {best_val_accuracy} as: Models/best_rnn_model.h5', flush=True)\n",
    "    best_model.save(\"Models/best_rnn_model.h5\")\n",
    "    \n",
    "    # Evaluate the best model's performance\n",
    "    print(\"\\n\\n4) Evaluation\")\n",
    "    evaluate_model(best_model, history, X_val_fold, y_val_fold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38d06a24-fd50-430f-8006-6d924821ec1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(best_model, X_val_fold, y_val_fold, emotion_labels):\n",
    "    # Get the model's predictions\n",
    "    y_pred_probs = best_model.predict(X_val_fold)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "    # Convert one-hot encoded y_val_fold back to label indices\n",
    "    y_true = np.argmax(y_val_fold, axis=1)\n",
    "\n",
    "    # Compute the confusion matrix\n",
    "    cm_data = confusion_matrix(y_true, y_pred, labels=range(len(emotion_labels)))\n",
    "    \n",
    "    # Convert the confusion matrix to a DataFrame for visualization\n",
    "    cm = pd.DataFrame(cm_data, columns=emotion_labels, index=emotion_labels)\n",
    "    cm.index.name = 'Actual'\n",
    "    cm.columns.name = 'Predicted'\n",
    "    \n",
    "    # Plot the confusion matrix\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.title('Confusion Matrix', fontsize=20)\n",
    "    sns.set(font_scale=1.2)\n",
    "    ax = sns.heatmap(cm, cbar=False, cmap=\"Blues\", annot=True, annot_kws={\"size\": 16}, fmt='g')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47c98327-e176-4dfe-8464-d21adbd523a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_model(best_model, history, X_val_fold, y_val_fold):\n",
    "    \n",
    "    y_pred = best_model.predict(X_val_fold)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_true = np.argmax(y_val_fold, axis=1)\n",
    "\n",
    "    while True:\n",
    "        print(\"\\n    Choose an evaluation option:\")\n",
    "        print(\"\\t1) Evaluate on validation set\")\n",
    "        print(\"\\t2) Plot training loss vs validation loss graph\")\n",
    "        print(\"\\t3) Plot training accuracy vs validation accuracy graph\")\n",
    "        print(\"\\t4) Print confusion matrix\")\n",
    "        print(\"\\t5) Print classification report\")\n",
    "        print(\"\\t6) Exit evaluation\")\n",
    "\n",
    "        choice = input(\"\\nEnter your choice (1/2/3/4/5/6): \")\n",
    "\n",
    "        if choice == \"1\":\n",
    "            accuracy = np.mean(y_true == y_pred_classes)\n",
    "            print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")\n",
    "        elif choice == \"2\":\n",
    "            plt.plot(history.history[\"loss\"],'r', label=\"Training Loss\")\n",
    "            plt.plot(history.history[\"val_loss\"],'b', label=\"Validation Loss\")\n",
    "            plt.legend()\n",
    "            plt.show() \n",
    "            # plot_loss(history)\n",
    "        elif choice == \"3\":\n",
    "            plt.plot(history.history[\"accuracy\"],'r',label=\"Training Accuracy\")\n",
    "            plt.plot(history.history[\"val_accuracy\"],'b',label=\"Validation Accuracy\")\n",
    "            plt.legend()\n",
    "            plt.show() \n",
    "            # plot_accuracy(history)\n",
    "        elif choice == \"4\":\n",
    "             plot_confusion_matrix(best_model, X_val_fold, y_val_fold, emotion_labels)\n",
    "        elif choice == \"5\":\n",
    "            print(\"Classification Report:\")\n",
    "            print(classification_report(y_true, y_pred_classes))\n",
    "        elif choice == \"6\":\n",
    "            break\n",
    "        else:\n",
    "            print(\"Invalid choice. Please enter a number between 1 and 5.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
